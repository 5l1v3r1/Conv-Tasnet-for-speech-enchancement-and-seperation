nohup: ignoring input
2019-02-11 17:56:37,725 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:27 - INFO ] TasNET:
TasNET(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(20,), stride=(10,), bias=False)
  )
  (separator): TemporalConvNet(
    (network): Sequential(
      (0): ChannelwiseLayerNorm()
      (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
      (2): Sequential(
        (0): Sequential(
          (0): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (1): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (2): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (3): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (4): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (5): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (6): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (7): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
        )
        (1): Sequential(
          (0): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (1): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (2): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (3): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (4): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (5): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (6): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (7): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
        )
        (2): Sequential(
          (0): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (1): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (2): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (3): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (4): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (5): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (6): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (7): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
        )
        (3): Sequential(
          (0): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (1): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (2): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (3): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (4): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (5): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (6): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
          (7): TemporalBlock(
            (net): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
              (1): PReLU(num_parameters=1)
              (2): GlobalLayerNorm()
              (3): DepthwiseSeparableConv(
                (net): Sequential(
                  (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                  (1): PReLU(num_parameters=1)
                  (2): GlobalLayerNorm()
                  (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
                )
              )
            )
          )
        )
      )
      (3): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
    )
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=20, bias=False)
  )
)
2019-02-11 17:56:37,726 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:30 - INFO ] Transfrom lr from str to float => 0.001
2019-02-11 17:56:41,011 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:46 - INFO ] Clip gradient by 2-norm 3
2019-02-11 17:56:41,013 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/utils.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).
  frame = signal.new_tensor(frame).long()  # signal may in GPU or CPU
2019-02-11 17:58:33,261 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:139 - INFO ] Start training for 100 epoches
2019-02-11 17:58:33,261 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:140 - INFO ] Epoch  0: dev loss =6.9802e+01
2019-02-11 17:58:33,324 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 18:24:21,757 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 18:26:07,296 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  1: train loss = -1.3215e+01(1548.43s/3238) | dev loss= -1.8653e+01(105.54s/815)
2019-02-11 18:26:07,347 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 18:52:09,450 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 18:53:55,251 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  2: train loss = -2.0875e+01(1562.10s/3238) | dev loss= -2.2146e+01(105.80s/815)
2019-02-11 18:53:55,303 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 19:19:49,595 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 19:21:35,551 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  3: train loss = -2.3247e+01(1554.29s/3238) | dev loss= -2.3563e+01(105.96s/815)
2019-02-11 19:21:35,601 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 19:47:41,119 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 19:49:26,557 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  4: train loss = -2.4781e+01(1565.52s/3238) | dev loss= -2.5058e+01(105.44s/815)
2019-02-11 19:49:26,609 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 20:15:20,827 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 20:17:06,182 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  5: train loss = -2.5879e+01(1554.22s/3238) | dev loss= -2.5899e+01(105.36s/815)
2019-02-11 20:17:06,232 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 20:43:14,013 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 20:45:00,147 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  6: train loss = -2.6778e+01(1567.78s/3238) | dev loss= -2.6473e+01(106.14s/815)
2019-02-11 20:45:00,200 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 21:11:09,682 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 21:12:55,103 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  7: train loss = -2.7405e+01(1569.48s/3238) | dev loss= -2.7371e+01(105.42s/815)
2019-02-11 21:12:55,154 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 21:38:59,756 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 21:40:44,474 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  8: train loss = -2.8107e+01(1564.60s/3238) | dev loss= -2.7394e+01(104.72s/815)
2019-02-11 21:40:44,526 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 22:06:40,282 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 22:08:26,233 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch  9: train loss = -2.8695e+01(1555.76s/3238) | dev loss= -2.8217e+01(105.95s/815)
2019-02-11 22:08:26,283 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 22:34:37,485 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 22:36:22,616 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 10: train loss = -2.9235e+01(1571.20s/3238) | dev loss= -2.8591e+01(105.13s/815)
2019-02-11 22:36:22,668 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 23:02:31,036 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 23:04:16,288 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 11: train loss = -2.9678e+01(1568.37s/3238) | dev loss= -2.8868e+01(105.25s/815)
2019-02-11 23:04:16,339 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 23:30:21,000 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-11 23:32:06,435 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 12: train loss = -3.0063e+01(1564.66s/3238) | dev loss= -2.8884e+01(105.44s/815)
2019-02-11 23:32:06,487 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-11 23:58:15,071 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 00:00:00,377 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 13: train loss = -3.0413e+01(1568.58s/3238) | dev loss= -2.9427e+01(105.31s/815)
2019-02-12 00:00:00,430 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 00:26:04,106 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 00:27:50,725 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 14: train loss = -3.0771e+01(1563.68s/3238) | dev loss= -2.9702e+01(106.62s/815)
2019-02-12 00:27:50,776 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 00:53:59,542 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 00:55:44,935 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 15: train loss = -3.1142e+01(1568.77s/3238) | dev loss= -2.9934e+01(105.40s/815)
2019-02-12 00:55:44,987 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 01:22:02,701 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 01:23:48,128 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 16: train loss = -3.1352e+01(1577.71s/3238) | dev loss= -3.0217e+01(105.43s/815)
2019-02-12 01:23:48,181 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 01:49:55,411 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 01:51:41,671 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 17: train loss = -3.1721e+01(1567.23s/3238) | dev loss= -3.0299e+01(106.26s/815)
2019-02-12 01:51:41,725 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 02:17:43,879 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 02:19:29,320 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 18: train loss = -3.1994e+01(1562.15s/3238) | dev loss= -3.0434e+01(105.44s/815)
2019-02-12 02:19:29,372 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 02:45:37,677 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 02:47:23,225 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 19: train loss = -3.2183e+01(1568.31s/3238) | dev loss= -3.0812e+01(105.55s/815)
2019-02-12 02:47:23,276 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 03:13:27,954 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 03:15:14,900 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 20: train loss = -3.2434e+01(1564.68s/3238) | dev loss= -3.0678e+01(106.95s/815)
2019-02-12 03:15:14,952 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 03:41:26,146 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 03:43:11,598 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 21: train loss = -3.2680e+01(1571.19s/3238) | dev loss= -3.1002e+01(105.45s/815)
2019-02-12 03:43:11,650 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 04:09:18,074 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 04:11:04,347 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 22: train loss = -3.2919e+01(1566.42s/3238) | dev loss= -3.1074e+01(106.27s/815)
2019-02-12 04:11:04,399 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 04:37:06,999 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 04:38:52,374 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 23: train loss = -3.3170e+01(1562.60s/3238) | dev loss= -3.1258e+01(105.38s/815)
2019-02-12 04:38:52,426 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 05:04:51,060 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 05:06:37,096 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 24: train loss = -3.3347e+01(1558.63s/3238) | dev loss= -3.1470e+01(106.04s/815)
2019-02-12 05:06:37,148 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 05:32:41,293 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 05:34:27,808 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 25: train loss = -3.3577e+01(1564.14s/3238) | dev loss= -3.1481e+01(106.52s/815)
2019-02-12 05:34:27,859 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 06:00:32,348 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 06:02:18,551 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 26: train loss = -3.3651e+01(1564.49s/3238) | dev loss= -3.1759e+01(106.20s/815)
2019-02-12 06:02:18,603 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 06:28:22,308 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 06:30:07,677 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 27: train loss = -3.3832e+01(1563.71s/3238) | dev loss= -3.1790e+01(105.37s/815)
2019-02-12 06:30:07,728 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 06:56:25,730 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 06:58:10,711 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 28: train loss = -3.4043e+01(1578.00s/3238) | dev loss= -3.1889e+01(104.98s/815)
2019-02-12 06:58:10,762 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 07:24:17,146 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 07:26:03,016 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 29: train loss = -3.4144e+01(1566.38s/3238) | dev loss= -3.2072e+01(105.87s/815)
2019-02-12 07:26:03,066 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 07:52:10,482 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 07:53:56,876 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 30: train loss = -3.4291e+01(1567.42s/3238) | dev loss= -3.1971e+01(106.40s/815)
2019-02-12 07:53:56,929 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 08:19:57,392 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 08:21:43,798 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 31: train loss = -3.4414e+01(1560.46s/3238) | dev loss= -3.2172e+01(106.41s/815)
2019-02-12 08:21:43,849 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 08:47:50,453 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 08:49:35,685 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 32: train loss = -3.4574e+01(1566.60s/3238) | dev loss= -3.2430e+01(105.23s/815)
2019-02-12 08:49:35,737 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 09:15:35,817 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 09:17:22,014 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 33: train loss = -3.4779e+01(1560.08s/3238) | dev loss= -3.2426e+01(106.20s/815)
2019-02-12 09:17:22,065 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 09:43:21,265 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 09:45:06,292 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 34: train loss = -3.4961e+01(1559.20s/3238) | dev loss= -3.2537e+01(105.03s/815)
2019-02-12 09:45:06,345 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 10:11:17,328 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 10:13:02,549 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 35: train loss = -3.5089e+01(1570.98s/3238) | dev loss= -3.2443e+01(105.22s/815)
2019-02-12 10:13:02,600 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 10:39:08,222 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 10:40:53,848 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 36: train loss = -3.5202e+01(1565.62s/3238) | dev loss= -3.2546e+01(105.63s/815)
2019-02-12 10:40:53,898 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 11:07:21,095 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 11:09:07,897 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 37: train loss = -3.5262e+01(1587.20s/3238) | dev loss= -3.2760e+01(106.80s/815)
2019-02-12 11:09:07,948 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 11:35:11,867 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 11:36:58,737 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 38: train loss = -3.5398e+01(1563.92s/3238) | dev loss= -3.2873e+01(106.87s/815)
2019-02-12 11:36:58,789 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 12:04:54,350 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 12:07:33,600 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 39: train loss = -3.5548e+01(1675.56s/3238) | dev loss= -3.2852e+01(159.25s/815)
2019-02-12 12:07:33,668 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 12:46:23,308 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 12:48:50,273 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 40: train loss = -3.5580e+01(2329.64s/3238) | dev loss= -3.2903e+01(146.97s/815)
2019-02-12 12:48:50,326 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 13:16:06,996 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 13:18:04,150 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 41: train loss = -3.5719e+01(1636.67s/3238) | dev loss= -3.2911e+01(117.16s/815)
2019-02-12 13:18:04,210 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 13:44:17,097 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 13:46:03,318 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 42: train loss = -3.5848e+01(1572.89s/3238) | dev loss= -3.2984e+01(106.22s/815)
2019-02-12 13:46:03,370 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 14:12:13,748 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 14:13:59,697 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 43: train loss = -3.5917e+01(1570.38s/3238) | dev loss= -3.3048e+01(105.95s/815)
2019-02-12 14:13:59,749 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 14:40:01,393 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 14:41:46,793 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 44: train loss = -3.5944e+01(1561.64s/3238) | dev loss= -3.3102e+01(105.40s/815)
2019-02-12 14:41:46,845 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 15:07:50,604 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 15:09:35,573 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 45: train loss = -3.6109e+01(1563.76s/3238) | dev loss= -3.3136e+01(104.97s/815)
2019-02-12 15:09:35,624 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 15:35:49,910 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 15:37:35,595 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 46: train loss = -3.6124e+01(1574.29s/3238) | dev loss= -3.3144e+01(105.69s/815)
2019-02-12 15:37:35,649 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 16:03:52,112 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 16:05:37,453 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 47: train loss = -3.6244e+01(1576.46s/3238) | dev loss= -3.3249e+01(105.34s/815)
2019-02-12 16:05:37,504 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 16:31:40,749 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 16:33:26,355 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 48: train loss = -3.6373e+01(1563.24s/3238) | dev loss= -3.3303e+01(105.61s/815)
2019-02-12 16:33:26,405 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 16:59:34,282 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 17:01:20,764 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 49: train loss = -3.6490e+01(1567.88s/3238) | dev loss= -3.3401e+01(106.48s/815)
2019-02-12 17:01:20,816 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 17:27:38,162 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 17:29:23,554 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 50: train loss = -3.6540e+01(1577.35s/3238) | dev loss= -3.3536e+01(105.39s/815)
2019-02-12 17:29:23,607 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 17:55:56,952 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 17:57:50,069 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 51: train loss = -3.6606e+01(1593.35s/3238) | dev loss= -3.3399e+01(113.12s/815)
2019-02-12 17:57:50,121 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 18:24:22,575 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 18:27:06,990 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 52: train loss = -3.6717e+01(1592.45s/3238) | dev loss= -3.3530e+01(164.42s/815)
2019-02-12 18:27:07,041 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 18:55:53,176 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 18:58:16,517 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 53: train loss = -3.6808e+01(1726.14s/3238) | dev loss= -3.3428e+01(143.34s/815)
2019-02-12 18:58:16,571 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 19:40:37,454 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 19:43:00,725 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 54: train loss = -3.6900e+01(2540.88s/3238) | dev loss= -3.3498e+01(143.27s/815)
2019-02-12 19:43:00,780 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
Epoch    53: reducing learning rate of group 0 to 5.0000e-04.
2019-02-12 20:22:02,368 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 20:24:07,171 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 55: train loss = -3.7513e+01(2341.59s/3238) | dev loss= -3.4045e+01(124.81s/815)
2019-02-12 20:24:07,223 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 21:01:03,935 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 21:03:27,077 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 56: train loss = -3.7679e+01(2216.71s/3238) | dev loss= -3.4029e+01(143.14s/815)
2019-02-12 21:03:27,136 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 21:45:42,475 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 21:48:05,915 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 57: train loss = -3.7790e+01(2535.34s/3238) | dev loss= -3.3971e+01(143.44s/815)
2019-02-12 21:48:05,971 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 22:24:37,813 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 22:26:47,850 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 58: train loss = -3.7854e+01(2191.84s/3238) | dev loss= -3.4003e+01(130.04s/815)
2019-02-12 22:26:47,903 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 23:07:02,623 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 23:09:25,837 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 59: train loss = -3.7919e+01(2414.72s/3238) | dev loss= -3.4044e+01(143.22s/815)
2019-02-12 23:09:25,892 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-12 23:51:27,052 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-12 23:53:32,545 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 60: train loss = -3.7983e+01(2521.16s/3238) | dev loss= -3.4038e+01(125.49s/815)
2019-02-12 23:53:32,597 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 00:27:58,638 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 00:30:21,951 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 61: train loss = -3.8043e+01(2066.04s/3238) | dev loss= -3.4003e+01(143.32s/815)
2019-02-13 00:30:22,007 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 01:12:48,889 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 01:15:13,001 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 62: train loss = -3.8084e+01(2546.88s/3238) | dev loss= -3.4016e+01(144.11s/815)
2019-02-13 01:15:13,053 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 01:54:47,083 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 01:56:52,586 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 63: train loss = -3.8132e+01(2374.03s/3238) | dev loss= -3.4004e+01(125.50s/815)
2019-02-13 01:56:52,638 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
Epoch    62: reducing learning rate of group 0 to 2.5000e-04.
2019-02-13 02:34:06,025 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 02:36:29,129 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 64: train loss = -3.8389e+01(2233.39s/3238) | dev loss= -3.4104e+01(143.11s/815)
2019-02-13 02:36:29,190 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 03:19:10,324 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 03:21:34,419 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 65: train loss = -3.8455e+01(2561.13s/3238) | dev loss= -3.4091e+01(144.10s/815)
2019-02-13 03:21:34,481 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 03:58:55,160 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 04:01:00,304 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 66: train loss = -3.8494e+01(2240.68s/3238) | dev loss= -3.4074e+01(125.15s/815)
2019-02-13 04:01:00,355 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 04:40:44,567 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 04:43:07,113 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 67: train loss = -3.8528e+01(2384.21s/3238) | dev loss= -3.4049e+01(142.55s/815)
2019-02-13 04:43:07,166 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 05:25:25,146 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 05:27:40,747 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 68: train loss = -3.8560e+01(2537.98s/3238) | dev loss= -3.4078e+01(135.60s/815)
2019-02-13 05:27:40,797 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
Epoch    67: reducing learning rate of group 0 to 1.2500e-04.
2019-02-13 06:02:19,824 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 06:04:33,439 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 69: train loss = -3.8689e+01(2079.03s/3238) | dev loss= -3.4092e+01(133.62s/815)
2019-02-13 06:04:33,492 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 06:46:42,203 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 06:49:05,332 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 70: train loss = -3.8719e+01(2528.71s/3238) | dev loss= -3.4079e+01(143.13s/815)
2019-02-13 06:49:05,389 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 07:28:56,510 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 07:31:04,956 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 71: train loss = -3.8738e+01(2391.12s/3238) | dev loss= -3.4086e+01(128.45s/815)
2019-02-13 07:31:05,010 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 08:07:44,842 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 08:10:07,616 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 72: train loss = -3.8756e+01(2199.83s/3238) | dev loss= -3.4078e+01(142.78s/815)
2019-02-13 08:10:07,673 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
Epoch    71: reducing learning rate of group 0 to 6.2500e-05.
2019-02-13 08:52:15,145 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 08:54:39,093 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 73: train loss = -3.8824e+01(2527.47s/3238) | dev loss= -3.4106e+01(143.95s/815)
2019-02-13 08:54:39,149 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
2019-02-13 09:31:59,052 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:112 - INFO ] Evaluating...
2019-02-13 09:34:04,975 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:154 - INFO ] Epoch 74: train loss = -3.8838e+01(2239.90s/3238) | dev loss= -3.4083e+01(125.93s/815)
2019-02-13 09:34:05,029 [/data00/home/labspeech_intern/wangkang/Tasnet/code/Conv-TasNet-myself/trainer.py:77 - INFO ] Training...
